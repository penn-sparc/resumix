frontend:
  host: "http://localhost:8501"
  port: 8501

backend:
  host: "http://localhost:8000"
  port: 8000

llm:
  use_model: "deepseek"
  deepseek:
    model: "deepseek-chat"
    url: "https://api.deepseek.com/v1/chat/completions"
  local:
    model: "gemma3:4b"
    url: "http://localhost:11434/api/generate"
  docker:
    model: "gemma3:4b"
    url: "http://host.docker.internal:11434/api/generate"
  teleai:
    url: "https://www.srdcloud.cn/api/acbackend/openchat/v1/chat/completions"
  silicon:
    model: "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"
    url: "https://api.siliconflow.cn/v1/chat/completions"

ocr:
  use_model: "paddleocr"
  easyocr:
    model: "easyocr"
    directory: "resumix/models/easyocr"
    gpu: False
  paddle:
    model: "paddleocr"

sentence_transformer:
  use_model: "paraphrase-multilingual-MiniLM-L12-v2"
  directory: "resumix/models/sentence_transformer"

  # use_easyocr: True
  # use_paddle: False
  # easyocr:
  #   directory: "resumix/models/easyocr"
  #   gpu: False
